# ¿Qué hemos hecho?
- Leer papers
- Descargar dataset
- Preprocesado y tokenización 1B
- TF-IDF (libreria sklearn o desarrollarlo a mano)
- Subir a nitrogeno los datos (shared-hdd).
- Instalar scapy (preguntar si sale mensaje raro).
- Preprocesado: aplicarlo a todos los datos.
- Clasificadores (tras la representación):
    - SVM (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
    - RF (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
# Tareas

- Tokenizacion: Convertir bytes de paquete a tokens (numeros enteros)
    - Partir el paquete en palabras/tokens. Longitud?
        - 2B: 0-65535, todavía estamos bien.
        - 4B u 8B: no.

- Modelo:
    - Representaciones:
        - Word2vec (libreria gensim):
            - Referencias: https://arxiv.org/abs/1301.3781
            
    
    - Redes neuronales (aproximaciones directas)
        - CNN
        - RNN (LSTM)
        - Transformer

- Programacion del modelo:
    - Frameworks: 
        - keras: el más alto nivel, pero el más estricto/cerrado.
        - pytorch: el más cacharreable, más usado a nivel de investigación, pero difícil.
        - jax: parecido a pytorch.

- Entrenarlo

- Evaluarlo:
    - **Metodología de evaluación/experimental**: fijar qué es train, qué es test, evitar mezclas (o que los modelos/representaciones puedan ver el test), ...
    - Target:
        - VPN vs NoVPN: Hecho y fácil
        - **Tipo (email, chat, ftps, audio, video)**
        - Aplicaciones (ftp, hangout, facebook, email)
    - Métricas: 
        - Accuracy
        - Precision
        - Recall
        - F1-score
        - AUC: tenemos que revisar esto.
    - Comparación
        - Variar parámetros
