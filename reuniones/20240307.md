# ¿Qué hemos hecho?
- Leer papers
- Descargar dataset
- Preprocesado y tokenización 1B
- TF-IDF (libreria sklearn o desarrollarlo a mano)
- Subir a nitrogeno los datos (shared-hdd).
- Instalar scapy (preguntar si sale mensaje raro).
- Preprocesado: aplicarlo a todos los datos.
- Tokenizacion
    - 1B
    - 2B: 0-65535, todavía estamos bien.
    - Byte Pair Encoding (https://github.com/karpathy/minbpe)
- Modelo:
    - Representaciones:
        - Word2vec (libreria gensim):
            - Referencias: https://arxiv.org/abs/1301.3781

- Clasificadores (tras la representación):
    - SVM (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
    - RF (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
        - https://www.kaggle.com/code/prashant111/random-forest-classifier-tutorial   
- Evaluación:
    - Train-test split
    - Label2 (vpn+tipo). Metricas RF = 0.45.

# Tareas

- Posteriori = Verosim * priori
    - Si queremos que nuestros clasificadores no tengan en cuenta el prior, hay que **balancear** las clases.
        - Downsampling: reducir al tamaño más pequeño.
        - Upsampling: multiples copias
            - Data augmentation: añadir muestras artificiales a los datos.
        - Eliminar clases improbables.
- Tokenizacion: Convertir bytes de paquete a tokens (numeros enteros)
    - Partir el paquete en palabras/tokens. Longitud?
        - 4B u 8B: no.

- **Cambio de paquete a paquete a por flujos.**
    - Implementado pero pendiente de pruebas.
    - Flujo o conexión: (**unidireccional** o bidireccional)
    - Longitud del registro...
        - Cortamos toda a longitud fija
        - Rellenamos las que sean muy cortas
    - Tamaño medio de paquete, numero de paquetes, total de bytes, duracion del flujo, media de tiempo entre paquetes, ...

- Modelo:
    - Redes neuronales (aproximaciones directas)
        - CNN
        - RNN (LSTM)
        - Transformer

- Programacion del modelo:
    - Frameworks: 
        - keras: el más alto nivel, pero el más estricto/cerrado.
        - pytorch: el más cacharreable, más usado a nivel de investigación, pero difícil.
        - jax: parecido a pytorch.

- Entrenarlo

- Evaluación:
    - **Matrices de confusión**

