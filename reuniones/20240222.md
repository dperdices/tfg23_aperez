# ¿Qué hemos hecho?
- Leer papers
- Descargar dataset
- Preprocesado y tokenización 1B
- TF-IDF (libreria sklearn o desarrollarlo a mano)
- Subir a nitrogeno los datos (shared-hdd).
- Instalar scapy (preguntar si sale mensaje raro).
- Preprocesado: aplicarlo a todos los datos.
- Clasificadores (tras la representación):
    - SVM (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
    - RF (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
- Evaluación:
    - Train-test split
    - Label2 (vpn+tipo). Metricas RF = 0.45.
# Tareas

- Tokenizacion: Convertir bytes de paquete a tokens (numeros enteros)
    - Partir el paquete en palabras/tokens. Longitud?
        - **2B**: 0-65535, todavía estamos bien.
        - Byte Pair Encoding (https://github.com/karpathy/minbpe) Ejemplo basic
        - 4B u 8B: no.

- Cambio de paquete a paquete a por flujos.

- Modelo:
    - Representaciones:
        - **Word2vec** (libreria gensim):
            - Referencias: https://arxiv.org/abs/1301.3781

    
    - Redes neuronales (aproximaciones directas)
        - CNN
        - RNN (LSTM)
        - Transformer

- Programacion del modelo:
    - Frameworks: 
        - keras: el más alto nivel, pero el más estricto/cerrado.
        - pytorch: el más cacharreable, más usado a nivel de investigación, pero difícil.
        - jax: parecido a pytorch.

- Entrenarlo

